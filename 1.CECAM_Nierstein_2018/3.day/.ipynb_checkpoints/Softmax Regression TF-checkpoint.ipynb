{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a one layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/Programs/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# do not forget to swapoff -a\n",
    "import numpy as np       # linear algebra\n",
    "import pylab as pl       # plots\n",
    "import tensorflow as tf  # now we are actually using it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28), (60000,))\n",
      "((10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "# choose wisely\n",
    "(train, label_train), (test, label_test) = tf.keras.datasets.mnist.load_data()\n",
    "#(train, label_train), (test, label_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# make sure your data is floating point\n",
    "test = test.astype(np.float32)\n",
    "train = train.astype(np.float32)\n",
    "\n",
    "# print shapes\n",
    "print(train.shape, label_train.shape)\n",
    "print(test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28), (60000,))\n",
      "((10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "# here we can subsample in the data set, depending on the RAM\n",
    "num_test, num_train = 10000, 60000\n",
    "test, label_test = test[:num_test], label_test[:num_test]\n",
    "train, label_train = train[:num_train], label_train[:num_train]\n",
    "\n",
    "# print shapes\n",
    "print(train.shape, label_train.shape)\n",
    "print(test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping is a cheap operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forget about the image shape\n",
    "train = train.reshape((-1, 784))\n",
    "test = test.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder is to pre-allocate the data shape, without stating the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the session graph\n",
    "X_tf = tf.placeholder(shape=[None, 784], dtype=tf.float32) # for each image we have 784 pixels\n",
    "Y_tf = tf.placeholder(shape=[None],      dtype=tf.int64)   # for each image we have one scalar label, int 32 does not work?\n",
    "A_tf = tf.Variable(np.zeros((784, 10)), dtype=tf.float32)\n",
    "\n",
    "a_tf = tf.Variable(np.zeros((10)),      dtype=tf.float32)\n",
    "F_tf = tf.add(tf.matmul(X_tf, A_tf), a_tf) #X*A+a, a is the translation A is the scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F=Ax+a$ belongs to R^10 \n",
    "\n",
    "softmax does softmax(y_j)=exp(y_j)/Q Q is the partition function.\n",
    "\n",
    "the distribution is the same if we shift the y_j by subtracting the biggest y or the mean we have seen, $\\mu$, therefore\n",
    "\n",
    "softmax(y_j-\\mu)=exp(y_j-mu)/Q\n",
    "\n",
    "\n",
    "then we take the logarithm\n",
    "\n",
    "\n",
    "log_softmax_F_tf =  (F_tf-M_tf)-log_norm_tf \n",
    "F_tf-M_tf is the norm, and the other \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map scalar labels onto one-hot encoded vectors\n",
    "L_tf = tf.one_hot(Y_tf, 10, dtype=tf.float32) #How many class labels I am expecting.\n",
    "\n",
    "# compute crossentrop_softmax_with logits\n",
    "M_tf = tf.reduce_max(F_tf)\n",
    "norm_tf = tf.expand_dims(tf.reduce_sum(tf.exp(F_tf-M_tf), axis=1), 1) #Perform the sum in all dimension.\n",
    "log_norm_tf = tf.where(norm_tf < 0, tf.zeros_like(norm_tf), tf.log(norm_tf))\n",
    "log_softmax_F_tf =  (F_tf-M_tf)-log_norm_tf \n",
    "\n",
    "loss_tf = tf.reduce_mean(tf.reduce_sum(-L_tf * log_softmax_F_tf, axis=1)) \n",
    "\n",
    "# let us define the non-differentiable accuracy as metric\n",
    "\n",
    "\n",
    "#Correctly predicted gives a one or a zero depending on if the prediction \n",
    "correctly_predicted_tf = tf.equal(tf.argmax(L_tf, axis=1), \n",
    "                                  tf.argmax(tf.nn.softmax(F_tf-M_tf), axis=1))\n",
    "metric_tf = tf.reduce_mean(tf.cast(correctly_predicted_tf, tf.float32)) \n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(1E-6)\n",
    "step_tf = optimizer.minimize(loss_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train loss and metric:', 0.38121295, 0.89566666)\n",
      "('train loss and metric:', 0.3419817, 0.90498334)\n",
      "('train loss and metric:', 0.32299325, 0.90955)\n",
      "('train loss and metric:', 0.3157634, 0.9107)\n",
      "('train loss and metric:', 0.30905077, 0.91463333)\n",
      "('train loss and metric:', 0.30273232, 0.91605)\n",
      "('train loss and metric:', 0.29724538, 0.91651666)\n",
      "('train loss and metric:', 0.29339275, 0.9188833)\n",
      "('train loss and metric:', 0.28978094, 0.92001665)\n",
      "('train loss and metric:', 0.29017457, 0.9201667)\n",
      "('train loss and metric:', 0.28775722, 0.91941667)\n",
      "('train loss and metric:', 0.28470683, 0.92183334)\n",
      "('train loss and metric:', 0.28373432, 0.9217)\n",
      "('train loss and metric:', 0.28138384, 0.92245)\n",
      "('train loss and metric:', 0.27971983, 0.92205)\n",
      "('train loss and metric:', 0.28191596, 0.9212833)\n",
      "('test loss and metric (accuracy):', 0.28628686, 0.9195)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    num_iterations, print_every, batch_size = 2**14, 2**10, 2**5 #Tuned to give results in a reasonable time\n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        indices = np.random.choice(train.shape[0], batch_size, replace=False)\n",
    "        X, Y = train[indices], label_train[indices]\n",
    "        \n",
    "        sess.run(step_tf, feed_dict={X_tf: X, Y_tf: Y}) #this is where the data could be transferred to a GPU. \n",
    "        #The dictionary is used because if there is no connection, tensorflow does not know the placeholder\n",
    "        \n",
    "        if iteration % print_every == print_every-1:\n",
    "            loss, metric = sess.run([loss_tf, metric_tf], feed_dict={X_tf: train, Y_tf: label_train})\n",
    "            print(\"train loss and metric:\",loss, metric)\n",
    "            \n",
    "    loss, metric = sess.run([loss_tf, metric_tf], feed_dict={X_tf: test, Y_tf: label_test})\n",
    "    print(\"test loss and metric (accuracy):\", loss, metric) #Accuracy is the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "392px",
    "left": "1033px",
    "right": "50px",
    "top": "5px",
    "width": "357px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
