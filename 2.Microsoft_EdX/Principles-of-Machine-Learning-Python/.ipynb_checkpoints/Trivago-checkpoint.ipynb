{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trusted\n",
    "Jupyter Server: local\n",
    "Python 3: Idle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "row_num: a number uniquely identifying each row.\n",
    "\n",
    "• locale: the platform of the session.\n",
    "\n",
    "• day_of_week: Mon-Fri, the day of the week of the session.\n",
    "\n",
    "• hour_of_day: 00-23, the hour of the day of the session.\n",
    "\n",
    "• agent_id: the device used for the session.\n",
    "\n",
    "• entry_page: describes the landing page of the session.\n",
    "\n",
    "• path_id_set: shows all the locations that were visited during the session.\n",
    "\n",
    "• traffic_type: indicates the channel the user cane through eg. search engine, email, ...\n",
    "\n",
    "• session_duration: the duration in seconds of the session.\n",
    "\n",
    "• hits: the number of interactions with the trivago page during the session.\n",
    "\n",
    "Task: Note that the column “hits” has missing values. Use this data to build a model that predicts the number of hits per session, depending on the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "import numpy.random as nr\n",
    "import scipy.stats as ss\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(df,cols):\n",
    "    \"\"\"\n",
    "    Plots a histogram of the desired numerical data\n",
    "\n",
    "    Args:\n",
    "        vals: data_frame column\n",
    "        lab: string containing the label\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        sns.distplot(df[col])\n",
    "        plt.title('Histogram of ' + col)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_box(data, col, col_y):\n",
    "    \"\"\"\n",
    "    data is a pandas dataframe\n",
    "    col is the column with the categorical values\n",
    "    col_y is the column with the quantitative values\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.boxplot(col, col_y, data = data)\n",
    "    plt.xlabel(col) \n",
    "    plt.ylabel(col_y)\n",
    "\n",
    "def frequency_table(data, cols):\n",
    "    \"\"\"\n",
    "    Builds a frequency table for the specific columns\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        print('\\n' + 'For column ' + col)\n",
    "        print(data[col].value_counts())\n",
    "        print('There are %s unique values' %data[col].unique().shape[0])\n",
    "\n",
    "def plot_bars(df, cols):\n",
    "    \"\"\"\n",
    "    Bars for categorical variables\n",
    "    df: data frame\n",
    "    cols: columns to plot\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        fig = plt.figure(figsize=(6,6)) \n",
    "        ax = fig.gca()    \n",
    "        counts = df[col].value_counts() \n",
    "        counts.plot.bar(ax = ax, color = 'blue') \n",
    "        ax.set_title('Counts' + col) \n",
    "        ax.set_xlabel(col) \n",
    "        ax.set_ylabel('Counts')\n",
    "        plt.show()\n",
    "    \n",
    "    return fig , ax \n",
    "\n",
    "def plot_scatter(df, cols, col_y , alpha = 1.0):\n",
    "    for col in cols:\n",
    "        fig = plt.figure(figsize=(7,6)) \n",
    "        ax = fig.gca() \n",
    "        df.plot.scatter(x = col, y = col_y, ax = ax, alpha = alpha)\n",
    "        ax.set_title('Scatter plot of ' + col_y + ' vs. ' + col) \n",
    "        ax.set_xlabel(col) \n",
    "        ax.set_ylabel(col_y)\n",
    "        plt.show()\n",
    "\n",
    "def path_splitter(data):\n",
    "    \"\"\"\n",
    "    Return the number of path_id\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, str):\n",
    "        n_ids = len(data.split(';'))\n",
    "    elif math.isnan(data):\n",
    "        n_ids = 0\n",
    "    else:\n",
    "        n_ids = 1\n",
    "    \n",
    "    return n_ids\n",
    "\n",
    "\n",
    "def plot_box(df, col, col_y = 'hits'):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.boxplot(col, col_y, data=df,ax = ax)\n",
    "    plt.xlabel(col) \n",
    "    plt.ylabel(col_y)\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def group_cat(df, col, threshold ):\n",
    "    \"\"\"\n",
    "    Groups categorical variables into a group called \"other\"\n",
    "    Args:\n",
    "    df:data_frame\n",
    "    col:column to group\n",
    "    threshold: threshold to decide if the variable is grouped or not\n",
    "    \"\"\"\n",
    "    frequencies = pd.DataFrame(df[col].value_counts())\n",
    "    frequencies = frequencies.reset_index()\n",
    "    frequencies.columns = [col, 'frequency']\n",
    "    total = frequencies['frequency'].sum()\n",
    "    frequencies['perc']= frequencies['frequency']/total\n",
    "    group = frequencies[col].loc[frequencies['perc']<threshold].values\n",
    "\n",
    "    return group, frequencies\n",
    "\n",
    "def cat_aggregation(x,group):\n",
    "    \"\"\"\n",
    "    Assigns the variable \"other\" to the variables in the group\n",
    "    \"\"\"\n",
    "    if x in group:\n",
    "        x = 'other'\n",
    "    else: \n",
    "        x = str(x)\n",
    "    return x\n",
    "\n",
    "def encode_string(cat_feature):\n",
    "    \"\"\"\n",
    "    Creates dummy variables out of categorical features and encodes them. \n",
    "    \"\"\"\n",
    "    ## First encode the strings to numeric categories\n",
    "    enc = preprocessing.LabelEncoder()\n",
    "    enc.fit(cat_feature)\n",
    "    enc_cat_feature = enc.transform(cat_feature)\n",
    "    ## Now, apply one hot encoding\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    encoded = ohe.fit(enc_cat_feature.reshape(-1,1))\n",
    "    return encoded.transform(enc_cat_feature.reshape(-1,1)).toarray()\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    ## First compute R^2 and the adjusted R^2\n",
    "    r2 = sklm.r2_score(y_true, y_predicted)\n",
    "    \n",
    "    ## Print the usual metrics and the R^2 values\n",
    "    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n",
    "    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n",
    "    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n",
    "    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n",
    "    print('R^2                    = ' + str(r2))\n",
    "\n",
    "def hist_resids(y_test, y_score):\n",
    "    ## first compute vector of residuals. \n",
    "    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n",
    "    ## now make the residual plots\n",
    "    sns.distplot(resids)\n",
    "    plt.title('Histogram of residuals')\n",
    "    plt.xlabel('Residual value')\n",
    "    plt.ylabel('count')\n",
    "\n",
    "def resid_qq(y_test, y_score):\n",
    "    ## first compute vector of residuals. \n",
    "    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n",
    "    ## now make the residual plots\n",
    "    ss.probplot(resids.flatten(), plot = plt)\n",
    "    plt.title('Residuals vs. predicted values')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residual')\n",
    "\n",
    "def resid_plot(y_test, y_score):\n",
    "    ## first compute vector of residuals. \n",
    "    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n",
    "    ## now make the residual plots\n",
    "    sns.regplot(y_score, resids, fit_reg=False)\n",
    "    plt.title('Residuals vs. predicted values')\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('MLDataScientistCaseStudyData2020.csv')\n",
    "df_original.columns = [\"row_num\",\"locale\",\"day_of_week\",\"hour_of_day\",\"agent_id\",\"entry_page\",\"path_id\",\"traffic_type\",\"session_duration\",\"hits\"]\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are wrong cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_original.astype(np.object) == '?').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing a quick search on trivago, checking the source code, \"locale\" seems to be related with the website, eg=[UK,DE,IT] etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('MLDataScientistCaseStudyData2020.csv')\n",
    "df_original.columns = [\"row_num\",\"locale\",\"day_of_week\",\"hour_of_day\",\"agent_id\",\"entry_page\",\"path_id\",\"traffic_type\",\"session_duration\",\"hits\"]\n",
    "\n",
    "\n",
    "df_goal = df_original.loc[df_original['hits'] == '\\\\N'] \n",
    "df_test = df_original.loc[df_original['hits']!= '\\\\N']\n",
    "\n",
    "# Now I can cast the hits into int32\n",
    "df_test['hits'] = df_test['hits'].astype('int64')\n",
    "\n",
    "# Deleting a couple of missing session duration\n",
    "df_test = df_test.loc[df_test['session_duration'] != '\\\\N'] \n",
    "df_test['session_duration'] = df_test['session_duration'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on 5% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.8\n",
    "df_reduced = df_test.sample(frac = 0.8)\n",
    "df_test.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_reduced[df_reduced['path_id'].notna()]\n",
    "df_incomplete = df_reduced[df_reduced['path_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which are the categorical variables\n",
    "cat_cols = [x  for x in df_complete.columns if pd.api.types.is_string_dtype(df_complete[x])]\n",
    "print(cat_cols)\n",
    "\n",
    "# Define which are the Numerical variables\n",
    "numeric_cols = [x  for x in df_complete.columns if pd.api.types.is_numeric_dtype(df_complete[x])]\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table(df_complete, df_complete.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Special care is required with path_id as the values have strings and floats. Moreover there are some rows without any value. I will assume that if there is non. The empty are floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['n_ids'] = df_complete['path_id'].apply(lambda x:path_splitter(x))\n",
    "df_complete.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table(df_complete, [\"n_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['n_ids'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to explore what happens with those with missing path_id...\n",
    "Fist lets find out how many are there, it is just 2 percent. therefore, maybe I can find a relation with others such that I could fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing = df_test['path_id'].isnull().sum()\n",
    "print(n_missing/len(df_test.index))\n",
    "\n",
    "n_missing_goal = df_goal['path_id'].isnull().sum()\n",
    "print(n_missing_goal/len(df_goal.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUilding a subset of the data that is complete, to study if I can perform any smart way of filling the gaps.\n",
    "using nearest neighbor to forward or\n",
    "backward fill those missing values.\n",
    "You can impute missing values by various methods,\n",
    "you can use mean, median.\n",
    "You could do some sort of interpolation\n",
    "like a trend value and there's\n",
    "much more sophisticated methods out there as\n",
    "well to impute missing values.\n",
    "For machine learning, the relationship of greatest interest is between the features and the label. It can also be useful to examine the relationships between features to determine if the features are co-variate or not. Such a procedure can prove more reliable than simply computing correlation when the relationship is nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"locale\", data=df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_box(df_complete, 'locale')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0,100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"traffic_type\", data=df_complete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_box(df_complete, 'traffic_type')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0,100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating categories of categorical variables to reduce the number. Categorical features or labels with too many unique categories will limit the predictive power of a machine learning model. Aggregating categories can improve this situation, sometime greatly. However, one must be careful. It only makes sense to aggregate categories that are similar in the domain of the problem. Thus, domain expertise must be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, frequency = group_cat(df_complete, 'traffic_type', 0.1)\n",
    "df_complete['traffic_type_aggr'] = df_complete['traffic_type'].apply(lambda x:cat_aggregation(x,group))\n",
    "sns.countplot(x=\"traffic_type_aggr\", data=df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_box(df_complete, 'traffic_type_aggr') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0,100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"agent_id\", data=df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_box(df_complete, 'agent_id')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0,100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, frequency = group_cat(df_complete, 'agent_id', 0.03)\n",
    "df_complete['agent_aggr'] = df_complete['agent_id'].apply(lambda x:cat_aggregation(x,group))\n",
    "sns.countplot(x=\"agent_aggr\", data=df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_box(df_complete, 'agent_aggr')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0,100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"entry_page\", data=df_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, frequency = group_cat(df_complete, 'entry_page', 0.01)\n",
    "df_complete['entry_aggr'] = df_complete['entry_page'].apply(lambda x:cat_aggregation(x,group))\n",
    "sns.countplot(x=\"entry_aggr\", data=df_complete)v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_box(df_complete, 'entry_aggr')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylim(0,100)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one might have an influence as the medians and some of them are skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming numeric variables\n",
    "To improve performance of machine learning models transformations of the values are often applied. Typically, transformations are used to make the relationships between variables more linear. In other cases, transformations are performed to make distributions closer to Normal, or at least more symmetric. These transformations can include taking logarithms, exponential transformations and power transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot(df_complete, ['session_duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['session_duration'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are some values with 0 seconds, actually most of them, I will apply log(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[['log_duration']] = df_complete[['session_duration']].applymap(np.log1p)\n",
    "hist_plot(df_complete, ['log_duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot(df_complete, ['n_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[['log_n_ids']] = df_complete[['n_ids']].applymap(math.log)\n",
    "hist_plot(df_complete, ['log_n_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot(df_complete, ['hour_of_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning ingo a cyclical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['sin_hour'] = np.sin(2*np.pi*df_complete['hour_of_day']/24)\n",
    "df_complete['cos_hour'] = np.cos(2*np.pi*df_complete['hour_of_day']/24)\n",
    "hist_plot(df_complete, ['sin_hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day of the week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_day =  {'Monday':0, 'Tuesday':1, 'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5,'Sunday':6}\n",
    "df_complete['day_of_week_num'] = [dict_day[x] for x in df_complete['day_of_week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['sin_day'] = np.sin(2*np.pi*df_complete['day_of_week_num']/7)\n",
    "df_complete['cos_day'] = np.cos(2*np.pi*df_complete['day_of_week_num']/7)\n",
    "df_complete.plot(x = 'day_of_week_num', y = 'cos_day', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hits(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot(df_complete, ['hits'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete[['log_hits']] = df_complete[['hits']].applymap(math.log)\n",
    "hist_plot(df_complete, ['log_hits'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
