{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Welcome to our first lab of Module 2! In this lab we're going to do some work with strings.\n",
                "\n",
                "So let's start by loading up our environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from text_analytics import text_analytics\n",
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "ai = text_analytics()\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This time we're going to work with articles about corruption. These are lead paragraphs from *The New York Times*. Some are about corruption and some aren't. But they are drawn from the same set of countries. So we load our data set in memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file = \"NYT.Corruption.gz\"\n",
                "file = os.path.join(ai.data_dir, file)\n",
                "df = pd.read_csv(file, index_col = 0)\n",
                "print(df)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We're working with *strings* today. So we'll grab one at random from the data set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = ai.print_sample(df)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "So here we have a raw string. It doesn't tell us anything about words.\n",
                "\n",
                "Here we're going to split this string into individual words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = line.split()\n",
                "print(line)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We notice that, if there is any punctuation, it is included inside of a word. So we have a series of cleaning steps change this. Let's get a new line and try it in sequence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = ai.print_sample(df)\n",
                "print(\"\\n\")\n",
                "line = ai.clean_wordclouds(line, stage = 1)\n",
                "print(line)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This removes stopwords. These are words that are so common they dilute the content of a text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = ai.print_sample(df)\n",
                "print(\"\\n\")\n",
                "line = ai.clean_wordclouds(line, stage = 2)\n",
                "print(line)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This makes everything lowercase as well."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = ai.print_sample(df)\n",
                "print(\"\\n\")\n",
                "line = ai.clean_wordclouds(line, stage = 3)\n",
                "print(line)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This has removed punctuation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = ai.print_sample(df)\n",
                "print(\"\\n\")\n",
                "line = ai.clean_wordclouds(line, stage = 4)\n",
                "print(line)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And this gets rid of any other non-linguistic material, like email addresses. So, we don't always need to use this step-by-step cleaning function. Mostly we'll use the code below to fully clean each line."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "line = ai.print_sample(df)\n",
                "print(\"\\n\")\n",
                "line = ai.clean_wordclouds(line, stage = 4)\n",
                "print(line)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And that's all for this lab. We've seen examples of what each pre-processing step looks like. For our purposes, you can use this code, *ai.clean()*, to take care of the cleaning. If you want to have a closer look, reference that function in the *text_analytics* package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}